{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c77bdd",
   "metadata": {},
   "source": [
    "*This jupyter notebook is part of Arizona State University's course CAS 523 (Methods for Complex Systems Science: Statistics and Dimensionality Reduction) and was written by Bryan Daniels.  It was last updated August 17, 2022.*\n",
    "\n",
    "*This assignment takes some inspiration from the Heavy-Tailed Distributions exercise in Quantitative Economics with Python, by Thomas J. Sargent & John Stachurski: https://python.quantecon.org/heavy_tails.html.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adc24bb",
   "metadata": {},
   "source": [
    "# Using the language of uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c41969",
   "metadata": {},
   "source": [
    "Probability distributions are used to precisely specify our expectations and uncertainty about complex systems.  In statistical learning, we often encounter a situation in which we wish to find a distribution that best represents the noisy variation in given data.  Here, we will practice fitting a distribution to data and using the language of statistics to communicate about our uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139a85d",
   "metadata": {},
   "source": [
    "## Load and plot the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6d772a",
   "metadata": {},
   "source": [
    "We will use typical python packages for numerical work and plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceac939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18}) # increases font size on plots\n",
    "from pathlib import Path # to handle file paths across all operating systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c8e31",
   "metadata": {},
   "source": [
    "Financial markets are a good example of complex systems for which uncertainty is a key concept.  Let's look at some financial data: specifically, how the price of a large corporation's stock changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081669ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol,start_date,end_date = 'AMZN','2015-1-1', '2022-8-1'\n",
    "data_filename = 'percent_changes_{}_{}_to_{}.pkl'.format(symbol,start_date,end_date)\n",
    "data_path = Path('data/yfinance/'+data_filename)\n",
    "\n",
    "# # use this code to download ticker data directly if you have the yfinance python package installed\n",
    "# import yfinance as yf\n",
    "# prices = yf.download(symbol,start_date,end_date)['Adj Close']\n",
    "# percent_changes = prices.pct_change()[1:] * 100\n",
    "# percent_changes.to_pickle(data_path)\n",
    "\n",
    "# otherwise, you can use the following code to load data that I saved for you\n",
    "percent_changes = pd.read_pickle(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9126d1c",
   "metadata": {},
   "source": [
    "Here we have loaded the day-to-day percent changes in the price of a share of Amazon's stock."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40464705",
   "metadata": {},
   "source": [
    "Check out what the data looks like by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98dcf43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c562ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_changes.plot()\n",
    "plt.ylabel('Percent change');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783b5fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_changes.plot.hist(bins=50)\n",
    "plt.xlabel('Percent change');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ef5987",
   "metadata": {},
   "source": [
    "❓ **Find the worst day in terms of percentage loss for Amazon during these years.  What percent of the stock's value was lost?** *Hint: `percent_changes.min()` will find the smallest value, and `percent_changes[percent_changes==val]` will find entries that have a given value. You can check your answer [here](https://www.washingtonpost.com/business/2022/04/29/markets-wall-street-april/).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c960834",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7087dd",
   "metadata": {},
   "source": [
    "❓ **Given the number of days in the dataset, roughly how likely do you think it is that Amazon's stock will lose 14% of its value tomorrow?** *Hint: I'm not looking for anything fancy here—just a very rough estimate.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b058be",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ec12f",
   "metadata": {},
   "source": [
    "## Fit a normal distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7854d9",
   "metadata": {},
   "source": [
    "How can we more precisely characterize our uncertainty about how we expect Amazon's stock price to change tomorrow?  What if I asked you to estimate the probability that there was a 20% loss or 20% gain?  What about 50%?  Even though there were never jumps that large in the dataset, it's certainly not impossible.  But roughly how likely?\n",
    "\n",
    "For this, we will assume a mathematical form for the probability distribution and fit its parameters to the data.  The simplest distribution we might fit is the normal (aka Gaussian) distribution.  A normal distribution corresponds to the assumption that stock price deviations are the result of summing many small, uncorrelated contributions.\n",
    "\n",
    "(Note that our approach of simply fitting a single probability distribution is, while useful, rather naive.  An expert in stock trading would certainly have a lot to say about how stock prices could be more predictable—for instance, large changes in price are much more likely after the release of earnings reports.  This is one of many examples we will see in this class of a tradeoff between simpler models that are easier to analyze and more complicated ones that *might* be able to make better predictions.)\n",
    "\n",
    "To fit a normal distribution to the data, we use `scipy`'s `stats.norm.fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06caaa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramsNormal = stats.norm.fit(percent_changes)\n",
    "print(paramsNormal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd212e9",
   "metadata": {},
   "source": [
    "The output `paramsNormal` is a list of best-fit parameters for the normal distribution.  Let's be a little more specific in our print statement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69819ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best-fit parameters: mean = {:1.5}, std. dev. = {:1.5}.\".format(*paramsNormal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b341a",
   "metadata": {},
   "source": [
    "The following code plots the resulting probability density and compares it to the (normalized) histogram of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e9ee4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_changes.plot.hist(bins=50,density=True)\n",
    "xs = np.linspace(-15,15,100)\n",
    "plt.plot(xs,[stats.norm.pdf(x,*paramsNormal) for x in xs],lw=5)\n",
    "plt.xlabel('Percent change')\n",
    "plt.ylabel('Probability density')\n",
    "plt.title('Normal distribution');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26e41f6",
   "metadata": {},
   "source": [
    "On this scale, it's hard to see exactly how likely those rare events are out in the \"tails\" of the distribution.  To better see the tails, we can put the densities on a logarithmic scale using `plt.yscale`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233f95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_changes.plot.hist(bins=50,density=True)\n",
    "xs = np.linspace(-15,15,100)\n",
    "plt.plot(xs,[stats.norm.pdf(x,*paramsNormal) for x in xs],lw=5)\n",
    "plt.xlabel('Percent change')\n",
    "plt.ylabel('Probability density')\n",
    "plt.title('Normal distribution')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6837b0",
   "metadata": {},
   "source": [
    "❓ **Characterize by eye how well the normal distribution fits the data.  How well does it do in the middle, closer to zero change?  What about larger changes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920f78d9",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90dca83",
   "metadata": {},
   "source": [
    "We can be more precise about how well the distribution fits by calculating the likelihood function.  This tells us how likely it is that our observed data (blue histogram above) would have been output if we were to randomly sample from the fit distribution (orange curve above).  `scipy` gives this to us in the form of a \"negative log-likelihood\" using the `nnlf` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c0bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlfNormal = stats.norm.nnlf(paramsNormal,percent_changes)\n",
    "print('The negative log-likelihood for the normal distribution is {:1.5}.'.format(nlfNormal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac663ea",
   "metadata": {},
   "source": [
    "This doesn't tell us much for now, but will be useful when we compare to other hypotheses for the distribution.  (When the likelihood is larger—corresponding to the negative log-likelihood being smaller—the fit is better.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd02033",
   "metadata": {},
   "source": [
    "## Fit heavy-tailed distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9cc6d",
   "metadata": {},
   "source": [
    "In an attempt to do better at fitting the tails of the distribution, we will try a couple of forms that have so-called \"heavy tails\"—that is, they allow for extreme values to be more likely than would be possible with the exponentially decaying form of the normal distribution.\n",
    "\n",
    "One possible heavy-tailed distribution is the log-normal, which is implemented in `scipy` as `stats.lognorm`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d763d584",
   "metadata": {},
   "source": [
    "❓ **Fit a log-normal distribution to the data, make a plot comparing its probability density function to a histogram of the data, and compute the negative log-likelihood.**  *Hint: `scipy.stats` is set up so that `stats.lognorm.fit` works exactly the same as `stats.norm.fit` above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594d09fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✳️ Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422fd08",
   "metadata": {},
   "source": [
    "Now we can also compute a likelihood ratio comparing likelihoods of the normal and log-normal distributions.  If we call the negative log-likelihood for the log-normal distribution `nlfLognormal`, the following code will compute $K$, the ratio of the two likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac29dc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Klognormal = np.exp(-nlfLognormal - (-nlfNormal))\n",
    "print('The (naive) likelihood ratio for selecting log-normal over normal is {:1.5}.'.format(Klognormal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c8f97a",
   "metadata": {},
   "source": [
    "(Note: I call this a \"naive\" likelihood ratio because it does not account for the fact that the log-normal distribution includes one extra parameter.  With the freedom of that extra parameter, we expect the log-normal to be able to fit *any* data slightly better, even if it were not truly a better description of the data.  We will return to this issue of \"overfitting\" later in the course.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7855af03",
   "metadata": {},
   "source": [
    "❓ **Briefly interpret this likelihood ratio.  How does $K$ translate into confidence in selecting one model over another?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb2ac10",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a409c454",
   "metadata": {},
   "source": [
    "Another heavy-tailed distribution that has been used to describe financial data is the Cauchy distribution, implemented in `scipy` as `stats.cauchy`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8387589b",
   "metadata": {},
   "source": [
    "❓ **Fit a Cauchy distribution to the data, make a plot comparing its probability density function to a histogram of the data, and compute the negative log-likelihood.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✳️ Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1986c",
   "metadata": {},
   "source": [
    "❓ **Compute the likelihood ratio $K$ comparing the Cauchy distribution to the normal distribution.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✳️ Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e701b0",
   "metadata": {},
   "source": [
    "❓ **Interpret your results in terms of the certainty with which the data can be said to favor one of these distributions.  Given that a simple \"random walk\" model for price changes would produce a normal distribution, what does this say about the random walk hypothesis?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb25c440",
   "metadata": {},
   "source": [
    "✳️ **Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176dfe7",
   "metadata": {},
   "source": [
    "❓ **Interpret your results in terms of the likelihood of large events.  In particular, roughly how much more likely is a 10% daily loss using the Cauchy distribution as compared to the normal distribution?  How might this impact a stock trader?** *Hint: No need for exact numbers here—feel free to take estimates by eye from your plots.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c49d96",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18a2f89",
   "metadata": {},
   "source": [
    "⚛️ **Bonus questions (for nothing but bragging rights):**\n",
    "* Do a similar analysis for the price of a different stock or cryptocurrency.  *Hints: I downloaded the Amazon stock price data using the `yfinance` package (see https://pypi.org/project/yfinance/). To install `yfinance`  at the command line, use `pip install yfinance --upgrade --no-cache-dir`.*\n",
    "* Cauchy and normal distributions are part of a larger family of \"stable\" distributions: https://en.wikipedia.org/wiki/Stable_distribution.  This is implemented in `scipy` as `stats.levy_stable`.  Use this package to test Benoit Mandelbrot's hypothesis that commodity (or stock) prices follow an alpha-stable distribution with $\\alpha \\approx 1.7$ (see https://en.wikipedia.org/wiki/Stable_distribution#Applications).  *Hint: This distribution takes much more computer time to fit.  I used a subsample of the data to run the fit in a reasonable amount of time.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda76a97",
   "metadata": {},
   "source": [
    "✴️ **Answer:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
