{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bbe7eb",
   "metadata": {},
   "source": [
    "*This jupyter notebook is part of Arizona State University's course CAS 523 (Methods for Complex Systems Science: Statistics and Dimensionality Reduction) and was written by Bryan Daniels.  It was last updated September 7, 2022.*\n",
    "\n",
    "*This assignment uses data, available [here](https://archive.ics.uci.edu/ml/datasets/wine), from the UCI Machine Learning Repository.  Data citation: Dua, D. and Graff, C. (2019). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7614d4",
   "metadata": {},
   "source": [
    "# Comparing techniques for finding structure in high-dimensional data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df51b6a",
   "metadata": {},
   "source": [
    "In this exercise, we will perform dimensionality reduction on data with a relatively simple, known structure.  The hope is to gain some intuition for how these techniques work so that you can use them in more complicated examples in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4032cb9",
   "metadata": {},
   "source": [
    "## Load relevant packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcb0dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "plt.rcParams.update({'font.size': 18}) # increases font size on plots\n",
    "from pathlib import Path # to handle file paths across all operating systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72016e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebea66ec",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaae41a",
   "metadata": {},
   "source": [
    "We will work here with data taken from a chemical analysis of 178 Italian wines.  These wines were derived from 3 different types (cultivars) of grapes.  Each wine was measured using 13 different quantitative tests.  \n",
    "\n",
    "Here we load the data from text files: (Note that we split off the \"Class identifier\" column from the rest of the data, imagining that we don't know beforehand which wine belongs to which class.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff6f750",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineAttributes = pd.read_csv(Path('data/Wines/wine-attributes.csv'))\n",
    "wineData = pd.read_csv(Path('data/Wines/wine.data'),names=wineAttributes.columns)\n",
    "# split off the class identifier column and transform into a useful color syntax\n",
    "wineClasses = wineData.pop('Class identifier').apply(lambda c: 'C{}'.format(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2c9816",
   "metadata": {},
   "source": [
    "Let's take a look at what is there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e879ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineData"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621a7719",
   "metadata": {},
   "source": [
    "Our goal will be to find structure in the data that could allow us to reduce the number of attributes we need to keep track of for describing each wine.  We'll try two tactics:\n",
    "1) **Clustering** will map individual wines into a few classes.  Then a good description of any wine is just to say which class it is in, ignoring the other details.  \n",
    "2) **Manifold learning** will map individual wines into a 2-dimensional space.  Then a good description of any wine consists of its two coordinates in this space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a38fc4",
   "metadata": {},
   "source": [
    "## Preliminary analysis and processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb3544c",
   "metadata": {},
   "source": [
    "Before we get to the fancier tools for dimensionality reduction, we will do some preliminary analysis to look for any obvious patterns.  *It is always a good idea to start simple in any data analysis project!*\n",
    "\n",
    "An easy way to get some basic statistics from the data is to use the `pandas` function `describe`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a659ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineData.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbbb1a2",
   "metadata": {},
   "source": [
    "I also like to look at things visually to begin.  It's possible that a single one of these attributes already has values that group into a few clusters.  We wouldn't see this from the above statistics, but we would if we plotted some histograms.  The following code plots a separate histogram for each attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb7138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for attribute in wineData.columns:\n",
    "    plt.figure()\n",
    "    plt.hist(wineData[attribute],bins=15)\n",
    "    plt.xlabel(attribute)\n",
    "    plt.ylabel('Number of wines')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda3ebd7",
   "metadata": {},
   "source": [
    "❓ **Do any patterns or groups stand out to you from this basic analysis?** *Hint: There's no wrong answer here.  We are just getting in the habit of looking carefully at your data before doing any complicated analysis.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53e028",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1f186d",
   "metadata": {},
   "source": [
    "Next, we will work with our data a bit to get it in a more useful form.\n",
    "\n",
    "Notice that the 13 attributes vary in their typical size: typical values of \"Nonflavanoid phenols\" are less than 1, but \"Proline\" values are typically larger than 500 and vary by 100s.  If we naively use these data in clustering algorithms that rely on distance measures, then differences in Proline will be overemphasized and differences in Nonflavanoid phenols will be ignored.  To put the different attributes on a similar scale, we will take the common step of normalizing (or \"standardizing\") the data: subtract off the mean of each attribute and divide by its standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1915359",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineDataNormed = (wineData - wineData.mean())/wineData.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a70e5",
   "metadata": {},
   "source": [
    "Now each attribute has a similar scale:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda1f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineDataNormed.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c9b61",
   "metadata": {},
   "source": [
    "## Run clustering algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb671ab4",
   "metadata": {},
   "source": [
    "Now we will run some standard clustering routines to try to find relevant groups of wines.\n",
    "\n",
    "Two standard algorithms that we discussed in lecture are \"k-means\" and \"agglomerative clustering\" (aka hierarchical clustering).  The following code runs these algorithms on the normalized data.  *Note that we are forcing the code here to produce exactly 3 clusters, and that each wine must belong to exactly one cluster (so-called \"hard\" clustering).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443d62fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_results = cluster.KMeans(n_clusters=3).fit(wineDataNormed)\n",
    "\n",
    "ac_results = cluster.AgglomerativeClustering(n_clusters=3).fit(wineDataNormed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539ce01",
   "metadata": {},
   "source": [
    "Each \"results\" object contains `labels_` that lists the group assigned to each wine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55135a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_results.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c1c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "ac_results.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d89560",
   "metadata": {},
   "source": [
    "Though the label for each group may not be the same using the two algorithms, you may notice some similarities in the groupings.  Let's try to visualize this a bit more intuitively using PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8465ce61",
   "metadata": {},
   "source": [
    "## Visualize using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ae583",
   "metadata": {},
   "source": [
    "The `fit_transform` method in `sklearn.PCA` will perform PCA and project the data along the principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45edba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_projections = PCA(n_components=2).fit_transform(wineDataNormed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2abd273",
   "metadata": {},
   "source": [
    "Plotting the data along the first two principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc583a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_projections[:,0],\n",
    "            pca_projections[:,1])\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038d5018",
   "metadata": {},
   "source": [
    "Then we can use the `c` argument of `plt.scatter` to set the colors of the points according to the labels from a clustering algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d4dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pca_projections[:,0],\n",
    "            pca_projections[:,1],\n",
    "            c=kmeans_results.labels_)\n",
    "plt.xlabel('Principal component 1')\n",
    "plt.ylabel('Principal component 2')\n",
    "plt.title('K-means clusters');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27776b6",
   "metadata": {},
   "source": [
    "❓ **Use similar code to plot the clustering results of agglomerative clustering.  Are the results similar to k-means?  Roughly what percentage of the wines are clustered differently by the two algorithms?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a291e4",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aad951",
   "metadata": {},
   "source": [
    "For these wines, we can also compare our clusters to the three cultivars of grapes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6c9245",
   "metadata": {},
   "source": [
    "❓ **Make a similar scatter plot with colors corresponding to the grape cultivar.  How well do our found clusters correspond to the known cultivars?**  *Hint: The cultivar of grape is contained in `wineClasses`, which I set up to be easily passed as the argument to `c` in `plt.scatter`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b930d8b",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c65c79d",
   "metadata": {},
   "source": [
    "## Visualize using t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d009955c",
   "metadata": {},
   "source": [
    "Finally, let's practice using a nonlinear manifold learning method on the same data.\n",
    "\n",
    "Here we'll use the \"t-distributed stochasic neighbor embedding\" (t-SNE) algorithm that we encountered in lecture.  The following code runs t-SNE using default parameters, which outputs a two-dimensional vector describing each wine: *(Note: You may get some warning messages that you can ignore. Also, the \"stochastic\" aspect of t-SNE means you will get somewhat different results each time you run the algorithm, so you may want to experiment with running it a few times.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfbf679",
   "metadata": {},
   "outputs": [],
   "source": [
    "wineTSNE = TSNE().fit_transform(wineDataNormed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f848d268",
   "metadata": {},
   "source": [
    "❓ **Analogousy to the PCA plots above, make a scatter plot using the output of t-SNE, first with all points the same color, and then with colors corresponding to one of the clustering algorithms.** *Hint: Don't forget to update your axis labels.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f817b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a59394",
   "metadata": {},
   "source": [
    "❓ **How do the dimensionality reduction results compare between PCA and t-SNE?  Are clusters more visible with t-SNE?  What does (nonlinear) t-SNE do with the \"curved arc\" of points identified by (linear) PCA?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521276e3",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d86c6",
   "metadata": {},
   "source": [
    "## Interpret the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6c1be",
   "metadata": {},
   "source": [
    "❓ **Using all the above evidence, briefly interpret the results in terms that a wine connoisseur might understand.  For instance: Are there clear differences between the three classes of wines?  Are some classes more distinguishable than others?  Do the data seem to show continuous variation among wines, or are there distinct separations between classes?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d0610",
   "metadata": {},
   "source": [
    "✳️ **Answer:** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49b883",
   "metadata": {},
   "source": [
    "⚛️ **Bonus question (for nothing but bragging rights): Experiment with other clustering methods (there are many available in `sklearn.cluster`), with varying the number of assumed clusters, and with varying parameters for t-SNE (particularly the number of dimensions and the \"perplexity\" parameter).  Can you gain any more insight into structure in the data?** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ba63c0",
   "metadata": {},
   "source": [
    "✴️ **Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b742871",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
